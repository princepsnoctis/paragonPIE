{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:44.983808700Z",
     "start_time": "2025-11-25T21:54:44.946303900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from doctest import master\n",
    "\n",
    "import torch"
   ],
   "id": "c2467ac0de13f995",
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:44.997948700Z",
     "start_time": "2025-11-25T21:54:44.984808300Z"
    }
   },
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, sym_len, max_seq_len, emb_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sym_len = sym_len\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.sym_embedding = torch.nn.Embedding(num_embeddings=sym_len,     embedding_dim=emb_dim)\n",
    "        self.pos_embedding = torch.nn.Embedding(num_embeddings=max_seq_len, embedding_dim=emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0) # (1, L)\n",
    "\n",
    "        sym_emb = self.sym_embedding(x)   # (B, L, D)\n",
    "        pos_emb = self.pos_embedding(pos) # (1, L, D)\n",
    "\n",
    "        x = sym_emb + pos_emb\n",
    "\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quick check",
   "id": "3f23db0684a14e1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:45.022135Z",
     "start_time": "2025-11-25T21:54:44.998948800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "sym_len = 90\n",
    "max_sam_len = 50\n",
    "\n",
    "sample = torch.randint(sym_len, (batch_size, max_sam_len)).cpu()\n",
    "print(sample.shape) # (8, 50)\n",
    "\n",
    "embedding = Embedding(sym_len=sym_len, max_seq_len=max_sam_len, emb_dim=512).cpu()\n",
    "\n",
    "x = embedding(sample)\n",
    "print(x.shape) # Expected (8, 50, 512)"
   ],
   "id": "907467a023e35bd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50])\n",
      "torch.Size([8, 50, 512])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:45.037511600Z",
     "start_time": "2025-11-25T21:54:45.023135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, emb_dim=512, num_heads=8, ff_dim=2048, dropout=0.1, num_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True, # More stable training\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            enable_nested_tensor=False, # For norm first\n",
    "        )\n",
    "\n",
    "        self.final_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x, sample_mask):\n",
    "        x = self.encoder(x, src_key_padding_mask=sample_mask)\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        return x"
   ],
   "id": "dc97dbd68e7931b8",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quick check",
   "id": "51ed327452416c17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:45.222584400Z",
     "start_time": "2025-11-25T21:54:45.038511500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = Encoder().cpu()\n",
    "\n",
    "sample_mask = torch.randint(2, (batch_size, max_sam_len), dtype=torch.bool).cpu()\n",
    "\n",
    "x = encoder(x, sample_mask=sample_mask)\n",
    "print(x.shape) # Expected (8, 50, 512)"
   ],
   "id": "279cd1fd7efdcc81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50, 512])\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:45.231845800Z",
     "start_time": "2025-11-25T21:54:45.224586300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, emb_dim=512, num_heads=8, ff_dim=2048, dropout=0.1, num_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        self.final_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, decoder_input, encoder_output, name_mask, sample_mask):\n",
    "        name_seq_len = decoder_input.shape[1]\n",
    "\n",
    "        causal_mask = torch.triu(\n",
    "            torch.full((name_seq_len, name_seq_len), float(\"-inf\"), device=decoder_input.device),\n",
    "            diagonal=1,\n",
    "        )\n",
    "\n",
    "        x = self.decoder(\n",
    "            tgt=decoder_input,\n",
    "            memory=encoder_output,\n",
    "            tgt_mask=causal_mask,\n",
    "            tgt_key_padding_mask=name_mask,\n",
    "            memory_key_padding_mask=sample_mask,\n",
    "        )\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        return x"
   ],
   "id": "652f9496e29e462",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Quick check",
   "id": "31fcf3233a2cb718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:54:45.414895500Z",
     "start_time": "2025-11-25T21:54:45.232845700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_nam_len = 25\n",
    "\n",
    "decoder_input = torch.randint(sym_len, (batch_size, max_nam_len)).cpu()\n",
    "print(decoder_input.shape) # (8, 25)\n",
    "\n",
    "decoder_embedding = Embedding(sym_len=sym_len, max_seq_len=max_nam_len)\n",
    "\n",
    "decoder_input = embedding(decoder_input)\n",
    "\n",
    "decoder = Decoder().cpu()\n",
    "\n",
    "name_mask = torch.randint(2, (batch_size, max_nam_len), dtype=torch.bool).cpu()\n",
    "\n",
    "print(\"####################################################\")\n",
    "print(x.shape)             # (8, 50, 512)\n",
    "print(sample_mask.shape)   # (8, 50)\n",
    "print(decoder_input.shape) # (8, 25, 512)\n",
    "print(name_mask.shape)     # (8, 25)\n",
    "\n",
    "y = decoder(\n",
    "    decoder_input=decoder_input,\n",
    "    encoder_output=x,\n",
    "    name_mask=name_mask,\n",
    "    sample_mask=sample_mask,\n",
    ")\n",
    "\n",
    "print(y.shape)"
   ],
   "id": "cd419576b23a5ef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 25])\n",
      "####################################################\n",
      "torch.Size([8, 50, 512])\n",
      "torch.Size([8, 50])\n",
      "torch.Size([8, 25, 512])\n",
      "torch.Size([8, 25])\n",
      "torch.Size([8, 25, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\princepsnoctis\\Desktop\\Paragon Roadmap\\ParagonNLP2\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
