{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training dataset structure",
   "id": "55048005f7f4f4b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "All data must follow one consistent format i.e.\n",
    "- always either \",\" or \".\" never both.\n",
    "- controlled unit vocabulary: KG, G, L, ML, SZT, etc. - never mixed SZt, szt, sZT\n",
    "- names should be standardized e.g.\n",
    "    - text only (i tried to follow this as much as possible but it turns out to be almost impossible)\n",
    "    - no tax letters\n",
    "    - no flags\n",
    "    - no quantity\n",
    "    - no price\n",
    "    - no total\n",
    "- missing data should be represented as a special symbol, empty labels produce ambiguity\n",
    "- corrupt labels should be dealt properly with special \\_NONE\\_ symbol (or for names, they should mimic linguistically meaningful names)"
   ],
   "id": "5e8c2e9bee8b23cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer design",
   "id": "22dec048207b0b45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vocabulary:\n",
    "- All possible characters\n",
    "- Special markers:\n",
    "    - <PAD> for padding\n",
    "    - <BOS> for beginning of sequence\n",
    "    - <EOS> for end of sequence\n",
    "    - <UNK> for unknown"
   ],
   "id": "dcf3f406eaf737c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Character-level approach is perfect fot his task because tokenizers struggle with:\n",
    "- merged words\n",
    "- broken words\n",
    "- hallucinated capitals\n",
    "- random digits in names\n",
    "- unknown brand names\n",
    "\n",
    "character-level model can freely cherrypick from noisy data"
   ],
   "id": "a39261d865f2b16e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Maximum input and output length should give safety without wasting memory",
   "id": "6f3777d22eff53e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "All sequences should be padded or cut to max length",
   "id": "e5d1d41fdde0d711"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoding should ignore special marks (aesthetic matter)",
   "id": "19d9a6c8c29ad248"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**IMPORTANT**: vocab must include special \\_NONE\\_ and \\n (newline) explicitly",
   "id": "a24d091fe6c9b37f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Architecture plan",
   "id": "18ebb6009a502452"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encoder --> Decoder --> Linear --> Softmax",
   "id": "9a7ac8b8eb479482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "No philosophy here, the \"philosophy\" is simply transformer is the best tool for seq2seq text",
   "id": "33b2ef5bc7d3067d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- **Embedding layer** maps each character ID to a dense vector, gives the model a continuous representation of characters\n",
    "- **Positional embedding** adds position information, without it transformer sees all characters as unordered\n",
    "- **Encoder** (4 layers)\n",
    "    - *Self-attention*\n",
    "        - Each characters looks at all other characters\n",
    "        - Learns dependencies like price patterns, spacing, noise patterns, word boundaries\n",
    "        - Essential for noisy OCR text\n",
    "    - *Feed-forward network* expands and compresses information, does non-linear feature extraction\n",
    "    - *Layer norm + residual connections*\n",
    "        - Stabilize training\n",
    "        - Allow deep stacking\n",
    "        - Prevent gradient collapse\n",
    "- **Decoder** analogical to encoder with few modifications:\n",
    "    - *Masked self-attention* when generating output, the model can only see the past tokens\n",
    "    - *Cross-attention* the decoder looks back at the encoder's output to decide what to spit out\n",
    "- **Output linear layer** maps decoder hidden state to vector of size of vocabulary\n",
    "- **Softmax** turns scores into probabilities, the model chooses the next character from this distribution"
   ],
   "id": "547d1c513712e2e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and loss design",
   "id": "f76baeeb917d7353"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Total loss as weighted sum:\n",
    "\n",
    "    TOTAL_LOSS =\n",
    "        w_name * CE(name) +\n",
    "        w_unit * CE(unit) +\n",
    "        w_amount * CE(w_amount) +\n",
    "        w_quantity * CE(w_quantity) +\n",
    "        w_price * CE(w_price) +\n",
    "        w_total * CE(w_total) +\n",
    "\n",
    "with example weights:\n",
    "\n",
    "    w_name = 1.0 Important\n",
    "    w_unit = 0.7\n",
    "    w_amount = 1.5 Critical\n",
    "    w_quantity = 1.5 Critical\n",
    "    w_price = 0.3\n",
    "    w_total = 0.3"
   ],
   "id": "a266d8ec9d1fcb55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Teacher forcing for all seq outputs is *ESSENTIAL* the *gold* previous char is precious, it stabilizes training",
   "id": "97f08af51c0499d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dropout should be used since it helps on small datasets so the model doesn't overfit",
   "id": "b6158e8522d48842"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Optional but cool metrics:\n",
    "- NAME accuracy, compare strings with Levenshtein distance\n",
    "- QUANTITY accuracy, exact class match\n",
    "- UNIT accuracy\n",
    "- PRICE accuracy, numeric difference after parsing to floats"
   ],
   "id": "5f492372f1edb50c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
